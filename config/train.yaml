seed: 12345
lr: 5.0e-6
model:
  name: "TransformerBaseline"
  text_encoder: "microsoft/deberta-v3-base"

trainer:
  gpus: 1
  max_epochs: 50
  precision: 16
  gradient_clip_val: 1.0
  val_check_interval: 1.0
  resume_from_checkpoint: null
  accumulate_grad_batches: 1
  num_sanity_val_steps: -1

# data module
data:
  batch_size: 16
  num_workers: 5
  seed: 42
  text_input: 'raw_sentence'
  use_definitions: false
  use_images: false
  val_percent: 0.2

# logger (tensorboard)
logger:
  name: null
  version: null
  save_dir: ~/logs/euphemism

# checkpoint
checkpoint:
  save_top_k: 1
  save_last: true
  verbose: true
  monitor: "f1"
  mode: "max"

# resume training
checkpoint_path: null

# hydra-specific
hydra:
  run:
    dir: .

defaults:
  - _self_
  - override hydra/job_logging: disabled